2023-12-18 22:05:58,765	INFO worker.py:1673 -- Started a local Ray instance.
Number of GPUs: 2
GPU Names:
  GPU 1: Quadro RTX 8000
  GPU 2: Quadro RTX 8000
Number of CPUs: 48
Files already downloaded and verified
Files already downloaded and verified
Data loading time: 2.02 seconds
Preprocessing time: 12.17 seconds
Traceback (most recent call last):
  File "/scratch/ssm10076/pytorch-example/Parallel-XGBoost/outputs/single_node/4-gpus/train-code.py", line 72, in <module>
    bst = train(
          ^^^^^^
  File "/home/ssm10076/.local/lib/python3.11/site-packages/xgboost_ray/main.py", line 1574, in train
    pg = _create_placement_group(
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ssm10076/.local/lib/python3.11/site-packages/xgboost_ray/main.py", line 965, in _create_placement_group
    raise TimeoutError(
TimeoutError: Placement group creation timed out after 100 seconds. Make sure your cluster either has enough resources or use an autoscaling cluster. Current resources available: {'node:10.32.35.74': 1.0, 'node:__internal_head__': 1.0, 'CPU': 48.0, 'object_store_memory': 112687532524.0, 'accelerator_type:RTX': 1.0, 'memory': 254375379968.0, 'GPU': 2.0}, resources requested by the placement group: [{'CPU': 2.0, 'GPU': 1.0}, {'CPU': 2.0, 'GPU': 1.0}, {'CPU': 2.0, 'GPU': 1.0}, {'CPU': 2.0, 'GPU': 1.0}]. You can change the timeout by setting the RXGB_PLACEMENT_GROUP_TIMEOUT_S environment variable.
